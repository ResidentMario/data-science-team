{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks in Manhattan?\n",
    "\n",
    "Our objective today is to map out and explore every Starbuck location in Manhattan. For the purposes of analysis, we \n",
    "will then use that list to compute average distance-to-Starbucks in Manhattan.\n",
    "\n",
    "## First, some background\n",
    "\n",
    "There are a few ways to get data off of the web: OCR, web scraping, APIs, and data stores are all valid approaches which are variously necessary for different specific problems. For our problem we will be using an API---specifically, the Yelp! API. We need a list of coordinates of chain store locations in Manhattan, and---lo and behold---Yelp! provides us with just such a list! We just have to figure out how to get it out of their hands into ours.\n",
    "\n",
    "For brevity, we are actually skipping one of the hardest parts of a data science project: figuring out what tools to use to approach the problem. Until you become a pro (and even often then) you will almost never have much of an idea about what you should do to answer a particular question, and will instead have to research your options and pick the best one (if one is available at all!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some boilerplate\n",
    "\n",
    "Let's import a giant pile of stuff! We'll talk about each of these in turn over the course of the presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yelp.client import Client\n",
    "from yelp.oauth1_authenticator import Oauth1Authenticator\n",
    "from yelp.errors import BusinessUnavailable\n",
    "import os\n",
    "import json\n",
    "from pandas import DataFrame\n",
    "import folium\n",
    "import geojson\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "from geopy.distance import vincenty\n",
    "import bokeh.plotting as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's authenticate with Yelp!.\n",
    "\n",
    "Here's the method we will use to import our credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_credentials(filename='yelp_credentials.json'):\n",
    "    \"\"\"\n",
    "    Finds the credentials file describing the token that's needed to access Yelp services.\n",
    "\n",
    "    :param filename -- The filename at which Yelp service credentials are stored. Defaults to\n",
    "    `yelp_credentials.json`.\n",
    "    \"\"\"\n",
    "    if filename in [f for f in os.listdir('.') if os.path.isfile(f)]:\n",
    "        data = json.load(open(filename))\n",
    "        return data\n",
    "    else:\n",
    "        raise IOError('This API requires Yelp credentials to work. Did you forget to define them?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you need to copy those credentials you have open in another tab (the ones on Yelp! - to reopen them jump to the top of this notebook and read the setup instructions again to get the link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = Oauth1Authenticator(\n",
    "    consumer_key='???',\n",
    "    consumer_secret='???',\n",
    "    token='???',\n",
    "    token_secret='???'\n",
    ")\n",
    "\n",
    "client = Client(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this actually doing? Stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core code\n",
    "\n",
    "**Now here comes the most important bit in this whole notebook. We're going to walk through this method line-by-line**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_businesses(name, area='New York'):\n",
    "    area = area.lower().replace(' ', '-')\n",
    "    name = name.lower().replace(' ', '-')\n",
    "    \"\"\"\n",
    "    Fetches all yelp.obj.business_response.BusinessResponse objects for incidences of the given chain in Manhattan.\n",
    "    Constructs Yelp business ids for incidences of the chain in the area, then queries Yelp to check if they\n",
    "    exist.\n",
    "    IDs are constructed name-location-number, so we just have to check numbers in ascending order until it breaks.\n",
    "    e.g. http://www.yelp.com/biz/gregorys-coffee-new-york-18 is good.\n",
    "         http://www.yelp.com/biz/gregorys-coffee-new-york-200 is not.\n",
    "    Then we do reverse GIS searches using the business ID through the Yelp API and extract coordinates from the results.\n",
    "    Some technical notes:\n",
    "    1.  The first incidence of any store in the area is reported without any numeral.\n",
    "        e.g. \"dunkin-donuts-new-york\", not \"dunkin-donuts-new-york-1\".\n",
    "        Numbers pick up from there: the next shitty hole in the wall will be \"dunkin-donuts-new-york-2\".\n",
    "    2.  Yelp IDs are unique and are not reassigned when a location is closed.\n",
    "        Thus we need to check for and exclude closed locations when munging the data.\n",
    "    3.  Places with a single instance in Manhattan sometimes have a \"name-place-2\" that redirects to their only location.\n",
    "        At least this seems to be the case with Bibble & Sip...\n",
    "        This is checked and corrected for further down the line, by the fetch_businesses() method.\n",
    "    4.  Sometimes IDs are given to locations that don't actually really exist.\n",
    "        e.g. the best-buy-3 id points to a non-existant storefront.\n",
    "        But best-buy-4, best-buy-5, and so on actually exist!\n",
    "        Yelp acknowledges this fact, but still returns a BusinessUnavaialable error when queries.\n",
    "        This method sends a web request and checks the response and terminates on a 404, which has proven to be a reliable\n",
    "        way of circumnavigating this issue.\n",
    "    5.  In case the above doesn't work...\n",
    "        The manual_override parameter forces the fetcher to keep moving past this error.\n",
    "        For debugging purposes, this method prints a URL for the purposes of manually checking breakpoints.\n",
    "        That way you can incrementally run fetch() and then comb over trouble spots you find by moving up manual_override.\n",
    "        If you hit that URL and you get either a valid ID or an invalid but existing ID, you need to bump up manual_override\n",
    "        to correct it and rerun the fetch.\n",
    "        If you hit that URL and you get a 404 page then you're done!\n",
    "        e.x. In the Best Buy case both best-buy-3 and best-buy-10 are phantoms.\n",
    "        But once we set manual_override=10 we're good, and get all of the actual storefronts.\n",
    "    \"\"\"\n",
    "    i = 2\n",
    "    # Run the first one through by hand.\n",
    "    try:\n",
    "        responses = [client.get_business(\"{0}-{1}\".format(name, area))]\n",
    "    # This can happen, and did, in the Dunkin' Donuts case.\n",
    "    except BusinessUnavailable:\n",
    "        responses = []\n",
    "        pass\n",
    "    # The rest are handled by a loop.\n",
    "    while True:\n",
    "        bus_id = \"{0}-{1}-{2}\".format(name, area, i)\n",
    "        try:\n",
    "            response = client.get_business(bus_id)\n",
    "        except BusinessUnavailable:\n",
    "            # We manually check trouble spots.\n",
    "            # But see the TODO.\n",
    "            if requests.get('http://www.yelp.com/biz/' + bus_id).status_code != requests.codes.ok:\n",
    "                break\n",
    "            else:\n",
    "                # Increment the counter but don't include the troubled ID.\n",
    "                i += 1\n",
    "                continue\n",
    "        responses += [response]\n",
    "        i += 1\n",
    "    print(\"Ended `fetch_businesses()` on:\", \"http://www.yelp.com/biz/\" + bus_id)\n",
    "    return responses     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Long explainy part in words, not in text*\n",
    "\n",
    "Let's try running this method on an example business. What do we get back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bas = fetch_businesses('Bibble and Sip')\n",
    "bas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need all of the nuts and bolts of the business entity, though all of that is present in these objects. We need just one piece of information: their coordinates. After reading some of the library's documentation (ommitted) you will find that to that you need a somewhat hideously long construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bas[0].business.location.coordinate.latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[bas[0].business.location.coordinate.latitude, bas[0].business.location.coordinate.longitude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! We can move on to our next chunky method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frame(responses):\n",
    "    \"\"\"\n",
    "    Given a list of yelp.obj.business_response.BusinessResponse objects like the one returns by fetch_businesses(),\n",
    "    builds a coordinate-logging DataFrame out of them.\n",
    "    \"\"\"\n",
    "    latitudes = [response.business.location.coordinate.latitude for response in responses]\n",
    "    longitudes = [response.business.location.coordinate.longitude for response in responses]\n",
    "    df = DataFrame({'latitude': latitudes, 'longitude': longitudes})\n",
    "    df.index.name=responses[0].business.name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame(bas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two locations are in fact one location!\n",
    "\n",
    "Data science is all about **edge cases**. This problem doesn't apply to chain stores - which is what we will be dealing with - so we can actually, yes, *ignore it*. Wow. Such duct tape.\n",
    "\n",
    "DataFrames like this one are the interpretive core of the Python data science stack, and you'll get very familiar with them very quickly as you go along.\n",
    "\n",
    "Next we'll write the Folium method we need to for visualization. Our DataFrame serves as an intermediary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_coordinates(df):\n",
    "    \"\"\"\n",
    "    Returns a folium map of all of the coordinates stored in a coordinate DataFrame, like the one returned by frame().\n",
    "    \"\"\"\n",
    "    ret = folium.Map(location=[40.753889, -73.983611], zoom_start=11)\n",
    "    for row in df.iterrows():\n",
    "        ret.simple_marker([row[1]['latitude'], row[1]['longitude']])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bas_df = frame(bas)\n",
    "map_coordinates(bas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this on something a little harder. You'll notice that this query takes a little while to run, because remember that in the background we're sending and reading requests to and from the web: this takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_coordinates(frame(fetch_businesses('Gregorys Coffee')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starbucks = fetch_businesses('Starbucks')\n",
    "map_coordinates(frame(starbucks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Measuring average distance\n",
    "\n",
    "Now that we've got our fancy Starbucks map we can jump into our true end goal: measuring distances. Hold on to your seatbelts, because this is going to be a wild ride!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain methodology and approach*\n",
    "\n",
    "Ok, let's look at the actual code.\n",
    "\n",
    "To start with, just take a quick look at [geojson.io](http://geojson.io).\n",
    "\n",
    "Ok, so let's assume I've generates this shapefile - [here is what my attempt came out to be](https://github.com/ResidentMario/chain-incidence/blob/master/manhattan.geojson). I then saved this file locally - if you followed the instructions at the beginning of this workshop, you will have it!\n",
    "\n",
    "These first two methods are for loading the shapefile and dumping the result into a list of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_geojson(filename=\"manhattan.geojson\"):\n",
    "    \"\"\"\n",
    "    Returns a geojson object for the given file.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        dat = f.read()\n",
    "        obj = geojson.loads(dat)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def load_coordinates(name):\n",
    "    \"\"\"\n",
    "    Loads Manhattan.\n",
    "    What else?\n",
    "    Are you surprised?\n",
    "    \"\"\"\n",
    "    # Encode according to our storage scheme.\n",
    "    filename = name.lower().replace(' ', '_') + '.geojson'\n",
    "    obj = load_geojson(filename)\n",
    "    if obj['type'] == 'FeatureCollection':\n",
    "        ret = []\n",
    "        for feature in obj['features']:\n",
    "            ret += list(geojson.utils.coords(feature))\n",
    "        # return ret\n",
    "    elif obj['type'] == 'Feature':\n",
    "        ret = list(geojson.utils.coords(obj))\n",
    "    # GeoJSON stores coordinates [Longitude, Latitude] -- the \"modern\" format.\n",
    "    # For historical reasons, coordinates are usually represented in the format [Latitude, Longitude].\n",
    "    # And this is indeed the format that the rest of the libraries used for this project expect.\n",
    "    # So we need to swap the two: [Longitude, Latitude] -> [Latitude, Longitude]\n",
    "    ret = [(coord[1], coord[0]) for coord in ret]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, a highly analytical point-in-polygon algorithm. This one we can treat as a black box - I never put in the time to figure out how it works, exactly, and instead just grabbed it off of elsewhere on the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def point_inside_polygon(x,y,poly):\n",
    "    \"\"\"\n",
    "    Checks if a point is inside a polygon.\n",
    "    Used to validate points as being inside of Manahttan.\n",
    "    Borrowed from: http://www.ariel.com.au/a/python-point-int-poly.html\n",
    "    \n",
    "    The shapely library provides features for this and other things besides, but is too much to deal with at the moment.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(poly)\n",
    "    inside = False\n",
    "\n",
    "    p1x,p1y = poly[0]\n",
    "    for i in range(n+1):\n",
    "        p2x,p2y = poly[i % n]\n",
    "        if y > min(p1y,p2y):\n",
    "            if y <= max(p1y,p2y):\n",
    "                if x <= max(p1x,p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\n",
    "                    if p1x == p2x or x <= xints:\n",
    "                        inside = not inside\n",
    "        p1x,p1y = p2x,p2y\n",
    "\n",
    "    return inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of coordinates which are boundaries of Manhattan and a method for checking if some random point is located in that shape. Here are the methods I use for generating a list of points which satisfy our conditions. I'll walk through these bits line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample_points(coordinate_list, n=1000):\n",
    "    \"\"\"\n",
    "    Generates n uniformly distributed sample points within the given coordinate list.\n",
    "    \n",
    "    When the geometry is sufficiently complex and the list of points large this query can take a while to process.\n",
    "    \"\"\"\n",
    "    lats, longs = list(map(lambda coords: coords[0], coordinate_list)), list(map(lambda coords: coords[1], coordinate_list))\n",
    "    max_lat = max(lats)\n",
    "    min_lat = min(lats)\n",
    "    max_long = max(longs)\n",
    "    min_long = min(longs)\n",
    "    ret = []\n",
    "    while True:\n",
    "        p_lat = random.uniform(min_lat, max_lat)\n",
    "        p_long = random.uniform(min_long, max_long)\n",
    "        if point_inside_polygon(p_lat, p_long, coordinate_list):\n",
    "            ret.append((p_lat, p_long))\n",
    "            if len(ret) > n:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sample_points(search_location, n=10000):\n",
    "    \"\"\"\n",
    "    Given the name of the location being search, returns n uniformally distributed points within that location.\n",
    "    \n",
    "    Wraps the above.\n",
    "    \"\"\"\n",
    "    return generate_sample_points(load_coordinates(search_location), n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we come up with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manhattan_point_cloud = sample_points(\"Manhattan\", n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.output_notebook(hide_banner=True)\n",
    "\n",
    "p = plt.figure(height=500,\n",
    "                width=960,\n",
    "                title=\"Manhattan Point Cloud\",\n",
    "                x_axis_label=\"Latitude\",\n",
    "                y_axis_label=\"Longitude\"\n",
    "               )\n",
    "\n",
    "p.scatter(\n",
    "    [coord[0] for coord in manhattan_point_cloud],\n",
    "    [coord[1] for coord in manhattan_point_cloud]\n",
    ")\n",
    "\n",
    "plt.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the methods that compress all of this splurge down to just the one number we want: average distance! Again let's walk through this line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minimum_distance(coordinate, coordinate_list):\n",
    "    \"\"\"\n",
    "    Naively calculates the minimum distance in the point cloud.\n",
    "    \"\"\"\n",
    "    best_coord = (0, 0)\n",
    "    best_distance = 1000\n",
    "    for candidate_coord in coordinate_list:\n",
    "        dist = vincenty(coordinate, candidate_coord).miles\n",
    "        if dist < best_distance:\n",
    "            best_coord = candidate_coord\n",
    "            best_distance = dist\n",
    "    return best_distance\n",
    "\n",
    "\n",
    "def average_distance(chain_name, search_location, point_cloud):\n",
    "    \"\"\"\n",
    "    This is the main function of this notebook!\n",
    "    Takes the name of the chain in question and the point cloud associated with the location\n",
    "    for which we are computing average distance.\n",
    "    Returns the average distance to that chain within that location.\n",
    "    We ask for a point cloud and not the name of the location because it's more efficient to precompute an extremely large,\n",
    "    essentially totally random point cloud, and then check against that, instead of recomputing it every round.\n",
    "    Output is in feet!\n",
    "    \"\"\"\n",
    "    # First load the coordinates corresponding to the search location..\n",
    "    location_coords = load_coordinates(search_location)\n",
    "    # Now generate a list of the chains' locations.\n",
    "    chain_df = frame(fetch_businesses(chain_name))\n",
    "    chain_coords = list(zip(chain_df['latitude'], chain_df['longitude']))\n",
    "    # Finally, get and average the minimum distances between the points in the point cloud and the chain locations.\n",
    "    distances = [get_minimum_distance(point, chain_coords) for point in point_cloud]\n",
    "    avg = sum(distances)/len(distances)\n",
    "    avg_in_feet = int(5280*avg)\n",
    "    return avg_in_feet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we are ready to generate our final answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg = average_distance(\"Starbucks\", \"Manhattan\", manhattan_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes this workshop!\n",
    "\n",
    "# What you need to be doing on your own\n",
    "\n",
    "Continue working through the starter tutorial list:\n",
    "\n",
    " 1. [Basic Python Course&mdash;Codeacademy](https://www.codecademy.com/learn/python).\n",
    " 2. [Crash Course in Numpy and Pandas](https://github.com/anabranch/data_analysis_with_python_and_pandas).\n",
    " 3. [Data Visualization in Pandas](http://pandas.pydata.org/pandas-docs/stable/visualization.html).\n",
    " 4. From here you should have enough knowledge to put into action and attempt your first project!\n",
    "\n",
    "Also: come up with an interesting question that you'd like to try to use data to answer. For inspiration, try leafing through [Dr. Randal Olson's blog](http://www.randalolson.com/blog/) or through [mine](http://www.residentmar.io/). We will start the next meeting off by discussing the problems that you guys come up with, and the techniques that we can use to address them. Who knows - if you come up with something good you can implement it and start off your data science portfolio!\n",
    "\n",
    "\n",
    "# What we will be doing next time\n",
    "\n",
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
